{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_honio3Yqjl2"
   },
   "source": [
    "# **Daily Minimum Temperatures Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hi·ªÉu d·ªØ li·ªáu (Dataset & Problem Definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG4pEApweeMi"
   },
   "source": [
    "### **T·∫£i b·ªô d·ªØ li·ªáu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2220,
     "status": "ok",
     "timestamp": 1753515899834,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "V2H_gNugelj6",
    "outputId": "a55a396e-9f18-41d9-e3f5-cc7972f134cf"
   },
   "outputs": [],
   "source": [
    "!gdown 1PWPwhW8QNOhPSOA1AtT7cu8-Uaxwo5UX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFEIxRlSs3N3"
   },
   "source": [
    "### **Import th∆∞ vi·ªán v√† load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1626,
     "status": "ok",
     "timestamp": 1753515901470,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "IUtg_mtCtDAw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªçc v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu Date sang d·∫°ng datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'timeseries_daily-minimum-temperatures.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1753515901584,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "wIgPwXSSs7m-",
    "outputId": "69738ecd-452b-4355-b10e-5de8d286b903"
   },
   "outputs": [],
   "source": [
    "# Read data from .csv file\n",
    "ts_df = pd.read_csv(DATASET_PATH, parse_dates=[\"Date\"])\n",
    "\n",
    "print(ts_df.shape)\n",
    "print(ts_df.dtypes)\n",
    "ts_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D·ªØ li·ªáu c√≥ 3650 d√≤ng, 2 c·ªôt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kh√¥ng c√≥ d√≤ng d·ªØ li·ªáu n√†o b·ªã thi·∫øu gi√° tr·ªã\n",
    "- Kh√¥ng c√≥ d√≤ng b·ªã tr√πng l·∫∑p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\", ts_df.isnull().sum().values[0])\n",
    "print(\"Duplicate rows:\", ts_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df['Date'] = pd.to_datetime(ts_df['Date'])\n",
    "# ts_df = ts_df.set_index('Date')\n",
    "ts_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = pd.read_csv(DATASET_PATH, index_col=0, parse_dates=True)\n",
    "ts_df['Year'] = ts_df.index.year\n",
    "ts_df['Month'] = ts_df.index.month\n",
    "ts_df['Weekday Name'] = ts_df.index.day_name()\n",
    "\n",
    "ts_df.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_df['Daily minimum temperatures'] = pd.to_numeric(\n",
    "    ts_df['Daily minimum temperatures'], errors='coerce')\n",
    "ts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Time-based indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = pd.read_csv(DATASET_PATH, index_col=0, parse_dates=True)\n",
    "\n",
    "# Bu·ªôc c·ªôt nhi·ªát ƒë·ªô v·ªÅ d·∫°ng s·ªë (float), chuy·ªÉn c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá (nh∆∞ chu·ªói header b·ªã s√≥t) th√†nh NaN.\n",
    "ts_df['Daily minimum temperatures'] = pd.to_numeric(\n",
    "    ts_df['Daily minimum temperatures'],\n",
    "    errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.loc['1990-01-01':'1991-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.loc['1990-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('1998-03-10', '1998-03-15', freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2004-09-20', periods=8, freq='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select an arbitrary sequence of date/time values from a pandas time series,\n",
    "# we need to use a DatetimeIndex, rather than simply a list of date/time strings\n",
    "times_sample = pd.to_datetime(['1981-01-01', '1981-01-04', '1981-01-08'])\n",
    "# Select the specified dates and just the Consumption column\n",
    "consum_sample = ts_df.loc[times_sample, ['Daily minimum temperatures']].copy()\n",
    "consum_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to daily frequency, without filling any missings\n",
    "consum_freq = consum_sample.asfreq('D')\n",
    "# Create a column with missings forward filled\n",
    "consum_freq['Daily minimum temperatures - Forward Fill'] = consum_sample.asfreq(\n",
    "    'D', method='ffill')\n",
    "consum_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kh√°m ph√° d·ªØ li·ªáu (Exploratory Data Analysis) v√† Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import YearLocator, DateFormatter\n",
    "\n",
    "# ==============================================================================\n",
    "# THI·∫æT L·∫¨P C·∫§U H√åNH TO√ÄN C·ª§C (GLOBAL CONFIGURATION)\n",
    "# M·ª•c ƒë√≠ch: T·∫≠p trung t·∫•t c·∫£ c√°c tham s·ªë c·ªßa M√¥ h√¨nh Sinusoidal v√† Ph√¢n t√≠ch\n",
    "# ==============================================================================\n",
    "CONFIG = {\n",
    "    # C·∫•u h√¨nh M√¥ h√¨nh Sinusoidal: y = A * sin(b*x + c) + d\n",
    "\n",
    "    # BASE_TEMP_D (d): Nhi·ªát ƒë·ªô trung b√¨nh/ƒê·ªô cao c∆° s·ªü h√†ng nƒÉm t·∫°i Melbourne: https://en.wikipedia.org/wiki/Climate_of_Melbourne\n",
    "    \"BASE_TEMP_D\": 9.7,\n",
    "\n",
    "    # AMPLITUDE_A (A): Bi√™n ƒë·ªô dao ƒë·ªông (L√Ω thuy·∫øt)\n",
    "    # D·ªØ li·ªáu l·∫•y t·ª´ Wikipedia (Trung b√¨nh t·ªëi thi·ªÉu h√†ng th√°ng t·∫°i S√¢n bay Melbourne):\n",
    "    # Nhi·ªát ƒë·ªô Min Max (Th√°ng 2) = 14.4 ¬∞C; Nhi·ªát ƒë·ªô Min Min (Th√°ng 7) = 5.8 ¬∞C.\n",
    "    # Ch√™nh l·ªách th·ª±c t·∫ø l√† (14.4 - 5.8) = 8.6 ¬∞C. Gi√° tr·ªã 8 ¬∞C ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ l√†m tr√≤n v√† l√†m m∆∞·ª£t bi√™n ƒë·ªô n√†y.\n",
    "    \"AMPLITUDE_A\": 8,\n",
    "\n",
    "    # DAYS_IN_YEAR (T): Chu k·ª≥ c·ªßa h√†m Sin (365.25 ng√†y/nƒÉm)\n",
    "    \"DAYS_IN_YEAR\": 365.25,\n",
    "\n",
    "    # DAYS_TO_SHIFT_PEAK (c - t√≠nh b·∫±ng ng√†y): D·ªãch pha\n",
    "    # Gi√° tr·ªã 60 ng√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ d·ªãch chuy·ªÉn ƒë·ªânh s√≥ng Sin (m√πa n√≥ng nh·∫•t) t·ª´\n",
    "    # v·ªã tr√≠ m·∫∑c ƒë·ªãnh (kho·∫£ng Th√°ng 4) v·ªÅ Th√°ng 2, ph√π h·ª£p v·ªõi d·ªØ li·ªáu kh√≠ h·∫≠u th·ª±c t·∫ø.\n",
    "    \"DAYS_TO_SHIFT_PEAK\": 60,\n",
    "\n",
    "    # C·∫•u h√¨nh K·ªπ thu·∫≠t Ph√¢n t√≠ch\n",
    "    \"ROLLING_WINDOW_DAYS\": 30,      # C·ª≠a s·ªï (s·ªë ng√†y) t√≠nh Trung b√¨nh Tr∆∞·ª£t\n",
    "\n",
    "    # C·∫•u h√¨nh Ng√¥n ng·ªØ/Hi·ªÉn th·ªã\n",
    "    \"MONTH_NAMES\": {\n",
    "        1: 'Th√°ng 1', 2: 'Th√°ng 2', 3: 'Th√°ng 3', 4: 'Th√°ng 4',\n",
    "        5: 'Th√°ng 5', 6: 'Th√°ng 6', 7: 'Th√°ng 7', 8: 'Th√°ng 8',\n",
    "        9: 'Th√°ng 9', 10: 'Th√°ng 10', 11: 'Th√°ng 11', 12: 'Th√°ng 12'\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- 1. T√çNH TO√ÅN C√ÅC H·∫∞NG S·ªê D·∫™N XU·∫§T (DERIVED CONSTANTS) ---\")\n",
    "# C√°c tham s·ªë ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi sang ƒë∆°n v·ªã radian cho h√†m np.sin()\n",
    "\n",
    "# ANGULAR_FREQUENCY_B (b): T·∫ßn s·ªë g√≥c = 2*pi / Chu k·ª≥ (365.25)\n",
    "CONFIG[\"ANGULAR_FREQUENCY_B\"] = (2 * np.pi) / CONFIG[\"DAYS_IN_YEAR\"]\n",
    "print(f\"T·∫ßn s·ªë g√≥c (b): {CONFIG['ANGULAR_FREQUENCY_B']:.6f}\")\n",
    "\n",
    "# PHASE_SHIFT_RADIAN_C (c - t√≠nh b·∫±ng radian): D·ªãch pha\n",
    "CONFIG[\"PHASE_SHIFT_RADIAN_C\"] = (2 * np.pi * CONFIG[\"DAYS_TO_SHIFT_PEAK\"]) / CONFIG[\"DAYS_IN_YEAR\"]\n",
    "print(f\"D·ªãch pha (c): {CONFIG['PHASE_SHIFT_RADIAN_C']:.6f} radians\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "print(\"\\n--- 2. PH·∫¶N 1: D·ªÆ LI·ªÜU TH·ª∞C V√Ä T√çNH TO√ÅN M√î H√åNH ---\")\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. D·ªÆ LI·ªÜU TH·ª∞C T·∫æ\n",
    "ts_df = pd.read_csv(DATASET_PATH, index_col=0, parse_dates=True)\n",
    "\n",
    "# Bu·ªôc c·ªôt nhi·ªát ƒë·ªô v·ªÅ d·∫°ng s·ªë (float), chuy·ªÉn c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá (nh∆∞ chu·ªói header b·ªã s√≥t) th√†nh NaN.\n",
    "ts_df['Daily minimum temperatures'] = pd.to_numeric(\n",
    "    ts_df['Daily minimum temperatures'],\n",
    "    errors='coerce'\n",
    ")\n",
    "# Lo·∫°i b·ªè c√°c d√≤ng b·ªã NaN sau khi √©p ki·ªÉu, ƒë·∫£m b·∫£o ch·ªâ c√≤n d·ªØ li·ªáu s·ªë h·ª£p l·ªá.\n",
    "ts_df = ts_df.dropna(subset=['Daily minimum temperatures'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. T√çNH TO√ÅN V√Ä TH√äM M√î H√åNH SINUSOIDAL (SIGNAL) V√ÄO ts_df\n",
    "# Ch·ªâ s·ªë th·ªùi gian (x): D·ª±a tr√™n ƒë·ªô d√†i d·ªØ li·ªáu th·ª±c t·∫ø ƒë√£ ƒë∆∞·ª£c t·∫£i\n",
    "time_index = np.arange(len(ts_df))\n",
    "\n",
    "# T√≠nh to√°n th√†nh ph·∫ßn t√≠n hi·ªáu PURE SIGNAL MODEL (S√≥ng Sin)\n",
    "model_signal = (\n",
    "    CONFIG[\"BASE_TEMP_D\"] +\n",
    "    CONFIG[\"AMPLITUDE_A\"] * np.sin(\n",
    "        CONFIG[\"ANGULAR_FREQUENCY_B\"] * time_index +\n",
    "        CONFIG[\"PHASE_SHIFT_RADIAN_C\"]\n",
    "    )\n",
    ")\n",
    "print(f\"ƒê√£ t√≠nh to√°n M√¥ h√¨nh Sinusoidal\")\n",
    "\n",
    "# Th√™m c·ªôt Signal v√†o DataFrame ƒë√£ t·∫£i ƒë·ªÉ so s√°nh tr·ª±c quan\n",
    "ts_df[\"Signal\"] = model_signal\n",
    "print(f\"ƒê√£ th√™m c·ªôt Signal (M√¥ h√¨nh L√Ω thuy·∫øt) v√†o DataFrame ts_df.\")\n",
    "\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. PH·∫¶N 2: PH√ÇN T√çCH CHU·ªñI TH·ªúI GIAN ---\")\n",
    "print(\"K·ªπ thu·∫≠t: Trung b√¨nh Tr∆∞·ª£t (Rolling Mean)\")\n",
    "# ==============================================================================\n",
    "# T√≠nh Trung b√¨nh Tr∆∞·ª£t tr√™n D·ªÆ LI·ªÜU TH·ª∞C T·∫æ\n",
    "ts_df['Rolling 30-Day Avg'] = ts_df['Daily minimum temperatures'].rolling(\n",
    "    window=CONFIG[\"ROLLING_WINDOW_DAYS\"],\n",
    "    center=True\n",
    ").mean()\n",
    "print(f\"ƒê√£ t√≠nh Trung b√¨nh Tr∆∞·ª£t {CONFIG['ROLLING_WINDOW_DAYS']}-Ng√†y cho D·ªØ li·ªáu Th·ª±c t·∫ø.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "print(\"\\n--- 4. PH·∫¶N 3: PH√ÇN T√çCH THEO M√ôA (MONTHLY ANALYSIS) ---\")\n",
    "# ==============================================================================\n",
    "\n",
    "# Th·ª±c hi·ªán ph√¢n t√≠ch tr√™n D·ªÆ LI·ªÜU TH·ª∞C T·∫æ\n",
    "ts_df['Month'] = ts_df.index.month\n",
    "monthly_avg = ts_df.groupby('Month')['Daily minimum temperatures'].mean().reset_index()\n",
    "monthly_avg['Month Name'] = monthly_avg['Month'].map(CONFIG[\"MONTH_NAMES\"])\n",
    "max_temp_month = monthly_avg.loc[monthly_avg['Daily minimum temperatures'].idxmax()]\n",
    "min_temp_month = monthly_avg.loc[monthly_avg['Daily minimum temperatures'].idxmin()]\n",
    "print(\"ƒê√£ th·ª±c hi·ªán nh√≥m d·ªØ li·ªáu theo Th√°ng v√† t√≠nh Trung b√¨nh m·ªói th√°ng tr√™n D·ªÆ LI·ªÜU TH·ª∞C T·∫æ.\")\n",
    "\n",
    "# In k·∫øt qu·∫£ ph√¢n t√≠ch ra Console\n",
    "print(\"\\n--- K·∫æT QU·∫¢ PH√ÇN T√çCH NHI·ªÜT ƒê·ªò TRUNG B√åNH THEO TH√ÅNG (D·ªØ li·ªáu Th·ª±c t·∫ø) ---\")\n",
    "print(f\"Th√°ng N√ìNG NH·∫§T (trung b√¨nh): {max_temp_month['Month Name']} ({max_temp_month['Daily minimum temperatures']:.2f} ¬∞C)\")\n",
    "print(f\"Th√°ng L·∫†NH NH·∫§T (trung b√¨nh): {min_temp_month['Month Name']} ({min_temp_month['Daily minimum temperatures']:.2f} ¬∞C)\")\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ph√¢n t√≠ch T√≠nh th·ªùi v·ª• (Seasonality Analysis) - EDA step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thi·∫øt l·∫≠p bi·ªÉu ƒë·ªì\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# V·∫º 1: D·ªÆ LI·ªÜU TH·ª∞C T·∫æ (ƒê·ªçc t·ª´ CSV)\n",
    "plt.plot(\n",
    "    ts_df.index,\n",
    "    ts_df[\"Daily minimum temperatures\"],\n",
    "    color=\"#0066CC\",\n",
    "    linewidth=1.0,\n",
    "    alpha=0.6,\n",
    "    label=\"1. D·ªØ li·ªáu Th·ª±c t·∫ø (CSV)\"\n",
    ")\n",
    "\n",
    "# V·∫º 2: M√î H√åNH L√ù THUY·∫æT (PURE SIGNAL) - ƒê∆∞·ªùng tham chi·∫øu S√≥ng Sin\n",
    "plt.plot(\n",
    "    ts_df.index,\n",
    "    ts_df[\"Signal\"],\n",
    "    color=\"#000000\",\n",
    "    linewidth=1.5,\n",
    "    linestyle='-',\n",
    "    label=\"2. M√¥ h√¨nh Sinusoidal (L√Ω thuy·∫øt)\"\n",
    ")\n",
    "\n",
    "# V·∫º 3: Trung b√¨nh tr∆∞·ª£t (Smoothed Data) - L√†m m∆∞·ª£t D·ªØ li·ªáu Th·ª±c t·∫ø\n",
    "plt.plot(\n",
    "    ts_df.index,\n",
    "    ts_df[\"Rolling 30-Day Avg\"],\n",
    "    color=\"#CC0000\",\n",
    "    linewidth=1.0,\n",
    "    linestyle='--',\n",
    "    label=f\"3. Trung b√¨nh tr∆∞·ª£t {CONFIG['ROLLING_WINDOW_DAYS']}-Ng√†y\"\n",
    ")\n",
    "\n",
    "# C·∫£i thi·ªán Tr·ª•c X (Th·ªùi gian)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(YearLocator())\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%Y'))\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Ti√™u ƒë·ªÅ v√† ch√∫ th√≠ch\n",
    "plt.title(\n",
    "    \"So s√°nh D·ªØ li·ªáu Th·ª±c t·∫ø (CSV) v√† M√¥ h√¨nh Sinusoidal Tinh khi·∫øt\",\n",
    "    fontsize=16,\n",
    "    fontweight='bold'\n",
    ")\n",
    "plt.xlabel(\"NƒÉm\", fontsize=12)\n",
    "plt.ylabel(\"Nhi·ªát ƒë·ªô (¬∞C)\", fontsize=12)\n",
    "plt.legend(loc='upper right', frameon=True)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "print(\"ƒê√£ v·∫Ω Bi·ªÉu ƒë·ªì Chu·ªói Th·ªùi gian (D·ªØ li·ªáu Th·ª±c t·∫ø v√† M√¥ h√¨nh).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫º BI·ªÇU ƒê·ªí THANH CHO PH√ÇN T√çCH THEO TH√ÅNG\n",
    "plt.figure(figsize=(10, 4))\n",
    "color_map = ['tab:red' if m == max_temp_month['Month Name'] else 'tab:blue' if m == min_temp_month['Month Name'] else 'tab:grey' for m in monthly_avg['Month Name']]\n",
    "\n",
    "plt.bar(\n",
    "    monthly_avg['Month Name'],\n",
    "    monthly_avg['Daily minimum temperatures'],\n",
    "    color=color_map\n",
    ")\n",
    "plt.title(\"Nhi·ªát ƒë·ªô T·ªëi thi·ªÉu Trung b√¨nh theo Th√°ng (D·ªØ li·ªáu Th·ª±c t·∫ø)\", fontsize=14)\n",
    "plt.xlabel(\"Th√°ng\", fontsize=12)\n",
    "plt.ylabel(\"Nhi·ªát ƒë·ªô TB (¬∞C)\", fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "print(\"ƒê√£ v·∫Ω Bi·ªÉu ƒë·ªì Thanh ph√¢n t√≠ch theo Th√°ng.\")\n",
    "\n",
    "# Hi·ªÉn th·ªã t·∫•t c·∫£ bi·ªÉu ƒë·ªì\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minh h·ªça d·ªØ li·ªáu theo t·ª´ng nƒÉm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1753515914503,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "1m4nB_2EYUYz",
    "outputId": "30724238-d2c9-4de7-9e68-4d30467ccc36"
   },
   "outputs": [],
   "source": [
    "ts_df = pd.read_csv(DATASET_PATH, index_col=0, parse_dates=True)\n",
    "\n",
    "# Bu·ªôc c·ªôt nhi·ªát ƒë·ªô v·ªÅ d·∫°ng s·ªë (float), chuy·ªÉn c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá (nh∆∞ chu·ªói header b·ªã s√≥t) th√†nh NaN.\n",
    "ts_df['Daily minimum temperatures'] = pd.to_numeric(\n",
    "    ts_df['Daily minimum temperatures'],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9, 4)})\n",
    "col_to_plot = 'Daily minimum temperatures'\n",
    "ts_df[col_to_plot].plot(linewidth=0.8)\n",
    "plt.ylabel(col_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1753515916002,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "zq_w-al0ZZaa",
    "outputId": "302d3a80-75eb-4cb1-e573-76cced3c2b12"
   },
   "outputs": [],
   "source": [
    "cols_plot = ['Daily minimum temperatures']\n",
    "ts_df[cols_plot].plot(\n",
    "    marker='.',\n",
    "    alpha=0.5,\n",
    "    linestyle='None',\n",
    "    figsize=(10, 4),\n",
    "    subplots=True\n",
    ")\n",
    "plt.ylabel('Daily minimum temperatures')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwNLOvdSvqoH"
   },
   "source": [
    "### **Ph√°t hi·ªán outliers**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1753515917986,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "_E2Ap5rMzQxh",
    "outputId": "6ae488fc-8251-4b54-cfe0-c2fc1f95542e"
   },
   "outputs": [],
   "source": [
    "ts_df = pd.read_csv(DATASET_PATH, index_col=0, parse_dates=True)\n",
    "ts_df['Year'] = ts_df.index.year\n",
    "ts_df['Month'] = ts_df.index.month\n",
    "ts_df['Weekday Name'] = ts_df.index.day_name()\n",
    "# Bu·ªôc c·ªôt nhi·ªát ƒë·ªô v·ªÅ d·∫°ng s·ªë (float), chuy·ªÉn c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá (nh∆∞ chu·ªói header b·ªã s√≥t) th√†nh NaN.\n",
    "ts_df['Daily minimum temperatures'] = pd.to_numeric(\n",
    "    ts_df['Daily minimum temperatures'],\n",
    "    errors='coerce'\n",
    ")\n",
    "COL_NAME = 'Daily minimum temperatures'\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(data=ts_df, x='Month', y=COL_NAME)\n",
    "plt.ylabel('Temperature')\n",
    "plt.title(COL_NAME)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resampling - Feature engineering** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1753515935293,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "4B1R0JyQ9_7D",
    "outputId": "d1be6d13-2c8f-4676-8bec-ad028b3b6268"
   },
   "outputs": [],
   "source": [
    "data_columns = ['Daily minimum temperatures']\n",
    "ts_weekly_mean = ts_df[data_columns].resample('W').mean()\n",
    "ts_weekly_mean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1753515936898,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "coAz-xtW-DuS",
    "outputId": "40b569f3-787e-48a1-e163-509651976ec8"
   },
   "outputs": [],
   "source": [
    "print(ts_df.shape[0])\n",
    "print(ts_weekly_mean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1753515938501,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "qgTIReBP-_ID",
    "outputId": "8741d814-5743-4004-f885-a3b8d1714d24"
   },
   "outputs": [],
   "source": [
    "# Start and end of the date range to extract\n",
    "start, end = '1981-01', '1981-12'\n",
    "# Plot daily and weekly resampled time series together\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ts_df.loc[start:end, 'Daily minimum temperatures'],\n",
    "marker='.', linestyle='-', linewidth=0.5, label='Daily')\n",
    "ax.plot(ts_weekly_mean.loc[start:end, 'Daily minimum temperatures'],\n",
    "marker='o', linestyle='-', label='Weekly Mean Resample')\n",
    "ax.set_ylabel('Temperature)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1753515939801,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "wYTd7VuqAM_p",
    "outputId": "2575aa39-eb77-4281-883a-86dd675b3612"
   },
   "outputs": [],
   "source": [
    "# Compute the annual sums, setting the value to NaN for any year which has\n",
    "# fewer than 360 days of data\n",
    "ts_annual_df = ts_df[data_columns].resample('A').sum(min_count=360)\n",
    "# The default index of the resampled DataFrame is the last day of each year\n",
    "# to the year component\n",
    "ts_annual_df = ts_annual_df.set_index(ts_annual_df.index.year)\n",
    "ts_annual_df.index.name = 'Year'\n",
    "# Compute the ratio of Wind+Solar to Consumption\n",
    "ts_annual_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1753515941635,
     "user": {
      "displayName": "AIO",
      "userId": "08245194604912763444"
     },
     "user_tz": -420
    },
    "id": "TMYa-nCCAbjM",
    "outputId": "e21559a8-e971-4986-efb3-e24eb3a49a8f"
   },
   "outputs": [],
   "source": [
    "# Plot from 1981 onwards, because there is no solar production data in earlier years\n",
    "ax = ts_annual_df.loc[1981:, 'Daily minimum temperatures'].plot.bar(color='C0')\n",
    "ax.set_ylabel('Fraction')\n",
    "ax.set_title('Annual Temperature')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chu·∫©n b·ªã d·ªØ li·ªáu cho training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "df = pd.read_csv('timeseries_daily-minimum-temperatures.csv')\n",
    "\n",
    "print(\"===== KI·ªÇM TRA D·ªÆ LI·ªÜU BAN ƒê·∫¶U =====\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nC√°c c·ªôt trong dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nTh√¥ng tin c√°c c·ªôt:\")\n",
    "print(df.info())\n",
    "print(f\"\\n5 d√≤ng ƒë·∫ßu:\")\n",
    "print(df.head())\n",
    "print(f\"\\nKi·ªÉu d·ªØ li·ªáu:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# T·ª± ƒë·ªông t√¨m t√™n c·ªôt nhi·ªát ƒë·ªô (c·ªôt s·ªë kh√¥ng ph·∫£i Date)\n",
    "date_col = None\n",
    "temp_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'date' in col_lower:\n",
    "        date_col = col\n",
    "    elif df[col].dtype in ['float64', 'int64', 'float32', 'int32']:\n",
    "        temp_col = col\n",
    "\n",
    "# N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ c·ªôt th·ª© 2\n",
    "if temp_col is None:\n",
    "    temp_col = df.columns[1]\n",
    "if date_col is None:\n",
    "    date_col = df.columns[0]\n",
    "\n",
    "print(f\"\\n‚úì C·ªôt Date: {date_col}\")\n",
    "print(f\"‚úì C·ªôt Temperature: {temp_col}\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "df[temp_col] = pd.to_numeric(df[temp_col], errors='coerce')\n",
    "\n",
    "# Lo·∫°i b·ªè missing values\n",
    "df = df.dropna()\n",
    "df = df.sort_values(date_col)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nShape sau khi x·ª≠ l√Ω: {df.shape}\")\n",
    "print(f\"Th·ªëng k√™ nhi·ªát ƒë·ªô:\")\n",
    "print(df[temp_col].describe())\n",
    "\n",
    "# ===== FEATURE ENGINEERING =====\n",
    "\n",
    "def create_time_series_features(df, target_col, date_col):\n",
    "    \"\"\"\n",
    "    T·∫°o c√°c features cho time series\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "\n",
    "    # 1. TIME-BASED FEATURES (ƒê·∫∑c tr∆∞ng th·ªùi gian)\n",
    "    df_features['year'] = df_features[date_col].dt.year\n",
    "    df_features['month'] = df_features[date_col].dt.month\n",
    "    df_features['day'] = df_features[date_col].dt.day\n",
    "    df_features['dayofweek'] = df_features[date_col].dt.dayofweek\n",
    "    df_features['dayofyear'] = df_features[date_col].dt.dayofyear\n",
    "    df_features['quarter'] = df_features[date_col].dt.quarter\n",
    "    df_features['weekofyear'] = df_features[date_col].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    # 2. CYCLIC FEATURES (ƒê·∫∑c tr∆∞ng tu·∫ßn ho√†n)\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['day'] / 31)\n",
    "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['day'] / 31)\n",
    "    df_features['dayofyear_sin'] = np.sin(2 * np.pi * df_features['dayofyear'] / 365)\n",
    "    df_features['dayofyear_cos'] = np.cos(2 * np.pi * df_features['dayofyear'] / 365)\n",
    "\n",
    "    # 3. LAG FEATURES (Gi√° tr·ªã qu√° kh·ª©)\n",
    "    for lag in [1, 2, 3, 7, 14, 30]:\n",
    "        df_features[f'temp_lag_{lag}'] = df_features[target_col].shift(lag)\n",
    "\n",
    "    # 4. ROLLING WINDOW FEATURES (ƒê·∫∑c tr∆∞ng c·ª≠a s·ªï tr∆∞·ª£t)\n",
    "    for window in [3, 7, 14, 30]:\n",
    "        df_features[f'temp_rolling_mean_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).mean()\n",
    "        df_features[f'temp_rolling_std_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).std()\n",
    "        df_features[f'temp_rolling_min_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).min()\n",
    "        df_features[f'temp_rolling_max_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).max()\n",
    "\n",
    "    # 5. EXPANDING WINDOW FEATURES (ƒê·∫∑c tr∆∞ng m·ªü r·ªông)\n",
    "    df_features['temp_expanding_mean'] = df_features[target_col].expanding(min_periods=1).mean()\n",
    "    df_features['temp_expanding_std'] = df_features[target_col].expanding(min_periods=1).std()\n",
    "\n",
    "    # 6. DIFFERENCE FEATURES (ƒê·∫∑c tr∆∞ng s·ª± thay ƒë·ªïi)\n",
    "    df_features['temp_diff_1'] = df_features[target_col].diff(1)\n",
    "    df_features['temp_diff_7'] = df_features[target_col].diff(7)\n",
    "\n",
    "    # 7. EXPONENTIAL WEIGHTED FEATURES\n",
    "    df_features['temp_ewm_mean_7'] = df_features[target_col].ewm(span=7, min_periods=1).mean()\n",
    "    df_features['temp_ewm_std_7'] = df_features[target_col].ewm(span=7, min_periods=1).std()\n",
    "\n",
    "    return df_features\n",
    "\n",
    "# T·∫°o features\n",
    "print(\"\\n===== T·∫†O FEATURES =====\")\n",
    "df_features = create_time_series_features(df, temp_col, date_col)\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng features: {len(df_features.columns)}\")\n",
    "print(f\"\\nDanh s√°ch features:\")\n",
    "for i, col in enumerate(df_features.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "# X·ª≠ l√Ω missing values\n",
    "print(f\"\\n\\nMissing values:\")\n",
    "missing_counts = df_features.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(missing_counts[missing_counts > 0].sort_values(ascending=False))\n",
    "    df_features = df_features.dropna()\n",
    "    print(f\"\\nShape sau khi x·ª≠ l√Ω missing values: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ missing values!\")\n",
    "\n",
    "# ===== CHU·∫®N B·ªä D·ªÆ LI·ªÜU CHO TRAINING =====\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PH√ÇN T√ÅCH FEATURES V√Ä TARGET\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# T√°ch features v√† target\n",
    "feature_columns = [col for col in df_features.columns if col not in [date_col, temp_col]]\n",
    "X_features = df_features[feature_columns].copy()\n",
    "y_target = df_features[temp_col].copy()\n",
    "\n",
    "print(f\"\\nüìä FEATURES (X_features):\")\n",
    "print(f\"   Shape: {X_features.shape}\")\n",
    "print(f\"   S·ªë l∆∞·ª£ng features: {len(feature_columns)}\")\n",
    "print(f\"\\n   5 d√≤ng ƒë·∫ßu ti√™n:\")\n",
    "print(X_features.head())\n",
    "\n",
    "print(f\"\\nüìä TARGET (y_target):\")\n",
    "print(f\"   Shape: {y_target.shape}\")\n",
    "print(f\"   T√™n c·ªôt: '{temp_col}'\")\n",
    "print(f\"\\n   5 gi√° tr·ªã ƒë·∫ßu ti√™n:\")\n",
    "print(y_target.head())\n",
    "\n",
    "print(f\"\\nüìà Th·ªëng k√™ y_target:\")\n",
    "print(y_target.describe())\n",
    "\n",
    "# ===== CHIA TRAIN/TEST THEO TH·ªúI GIAN =====\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PH√ÇN CHIA D·ªÆ LI·ªÜU THEO TH·ªúI GIAN\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Chia train/test theo th·ªùi gian (80/20)\n",
    "split_idx = int(len(X_features) * 0.8)\n",
    "X_train, X_test = X_features.iloc[:split_idx], X_features.iloc[split_idx:]\n",
    "y_train, y_test = y_target.iloc[:split_idx], y_target.iloc[split_idx:]\n",
    "\n",
    "# L·∫•y ng√†y th√°ng t∆∞∆°ng ·ª©ng\n",
    "train_start = df_features[date_col].iloc[0]\n",
    "train_end = df_features[date_col].iloc[split_idx-1]\n",
    "test_start = df_features[date_col].iloc[split_idx]\n",
    "test_end = df_features[date_col].iloc[-1]\n",
    "\n",
    "print(f\"\\nüìä K√≠ch th∆∞·ªõc:\")\n",
    "print(f\"   Train set: {X_train.shape[0]:,} m·∫´u ({X_train.shape[0]/len(X_features)*100:.1f}%)\")\n",
    "print(f\"   Test set:  {X_test.shape[0]:,} m·∫´u ({X_test.shape[0]/len(X_features)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìÖ Kho·∫£ng th·ªùi gian TRAIN:\")\n",
    "print(f\"   T·ª´: {train_start.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   ƒê·∫øn: {train_end.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nüìÖ Kho·∫£ng th·ªùi gian TEST:\")\n",
    "print(f\"   T·ª´: {test_start.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   ƒê·∫øn: {test_end.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# T√≠nh s·ªë ng√†y\n",
    "train_days = (train_end - train_start).days + 1\n",
    "test_days = (test_end - test_start).days + 1\n",
    "print(f\"\\n‚è±Ô∏è  ƒê·ªô d√†i:\")\n",
    "print(f\"   Train: {train_days:,} ng√†y\")\n",
    "print(f\"   Test:  {test_days:,} ng√†y\")\n",
    "\n",
    "# ===== CHU·∫®N H√ìA D·ªÆ LI·ªÜU =====\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CHU·∫®N H√ìA D·ªÆ LI·ªÜU (STANDARDIZATION)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ DataFrame ƒë·ªÉ gi·ªØ t√™n c·ªôt\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)\n",
    "\n",
    "print(f\"\\n‚úì X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"‚úì X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nüìä M·∫´u d·ªØ li·ªáu X_train_scaled (5 d√≤ng ƒë·∫ßu):\")\n",
    "print(X_train_scaled.head())\n",
    "\n",
    "print(f\"\\nüìä Th·ªëng k√™ sau chu·∫©n h√≥a (X_train_scaled):\")\n",
    "print(X_train_scaled.describe().loc[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# ===== KI·ªÇM TRA CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU =====\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"KI·ªÇM TRA CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Ki·ªÉm tra NaN\n",
    "nan_train = X_train_scaled.isnull().sum().sum()\n",
    "nan_test = X_test_scaled.isnull().sum().sum()\n",
    "nan_y_train = y_train.isnull().sum()\n",
    "nan_y_test = y_test.isnull().sum()\n",
    "\n",
    "print(f\"\\nüîç Missing Values (NaN):\")\n",
    "print(f\"   X_train_scaled: {nan_train}\")\n",
    "print(f\"   X_test_scaled: {nan_test}\")\n",
    "print(f\"   y_train: {nan_y_train}\")\n",
    "print(f\"   y_test: {nan_y_test}\")\n",
    "\n",
    "# Ki·ªÉm tra Inf\n",
    "inf_train = np.isinf(X_train_scaled.values).sum()\n",
    "inf_test = np.isinf(X_test_scaled.values).sum()\n",
    "\n",
    "print(f\"\\nüîç Infinite Values (Inf):\")\n",
    "print(f\"   X_train_scaled: {inf_train}\")\n",
    "print(f\"   X_test_scaled: {inf_test}\")\n",
    "\n",
    "if nan_train + nan_test + inf_train + inf_test + nan_y_train + nan_y_test == 0:\n",
    "    print(f\"\\n‚úÖ D·ªÆ LI·ªÜU S·∫†CH - Kh√¥ng c√≥ NaN ho·∫∑c Inf!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  C·∫¢NH B√ÅO: D·ªØ li·ªáu c√≥ v·∫•n ƒë·ªÅ c·∫ßn x·ª≠ l√Ω!\")\n",
    "\n",
    "# ===== T·ªîNG K·∫æT =====\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ FEATURE ENGINEERING HO√ÄN TH√ÄNH\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nüì¶ C√ÅC BI·∫æN ƒê√É T·∫†O:\")\n",
    "print(f\"   1. X_features: DataFrame ch·ª©a {X_features.shape[1]} features g·ªëc\")\n",
    "print(f\"   2. y_target: Series ch·ª©a target '{temp_col}'\")\n",
    "print(f\"   3. X_train_scaled: Features ƒë√£ chu·∫©n h√≥a cho training ({X_train_scaled.shape})\")\n",
    "print(f\"   4. X_test_scaled: Features ƒë√£ chu·∫©n h√≥a cho testing ({X_test_scaled.shape})\")\n",
    "print(f\"   5. y_train: Target cho training ({y_train.shape})\")\n",
    "print(f\"   6. y_test: Target cho testing ({y_test.shape})\")\n",
    "print(f\"   7. scaler: StandardScaler object (ƒë·ªÉ transform d·ªØ li·ªáu m·ªõi)\")\n",
    "print(f\"   8. df_features: DataFrame ƒë·∫ßy ƒë·ªß v·ªõi t·∫•t c·∫£ features\")\n",
    "\n",
    "print(f\"\\nüìå T√äN C·ªòT:\")\n",
    "print(f\"   - temp_col = '{temp_col}'\")\n",
    "print(f\"   - date_col = '{date_col}'\")\n",
    "\n",
    "print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "df = pd.read_csv('timeseries_daily-minimum-temperatures.csv')\n",
    "\n",
    "print(\"===== KI·ªÇM TRA D·ªÆ LI·ªÜU BAN ƒê·∫¶U =====\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nC√°c c·ªôt trong dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nTh√¥ng tin c√°c c·ªôt:\")\n",
    "print(df.info())\n",
    "print(f\"\\n5 d√≤ng ƒë·∫ßu:\")\n",
    "print(df.head())\n",
    "print(f\"\\nKi·ªÉu d·ªØ li·ªáu:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# T·ª± ƒë·ªông t√¨m t√™n c·ªôt nhi·ªát ƒë·ªô (c·ªôt s·ªë kh√¥ng ph·∫£i Date)\n",
    "date_col = None\n",
    "temp_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'date' in col_lower:\n",
    "        date_col = col\n",
    "    elif df[col].dtype in ['float64', 'int64', 'float32', 'int32']:\n",
    "        temp_col = col\n",
    "\n",
    "# N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ c·ªôt th·ª© 2\n",
    "if temp_col is None:\n",
    "    temp_col = df.columns[1]\n",
    "if date_col is None:\n",
    "    date_col = df.columns[0]\n",
    "\n",
    "print(f\"\\n‚úì C·ªôt Date: {date_col}\")\n",
    "print(f\"‚úì C·ªôt Temperature: {temp_col}\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "df[temp_col] = pd.to_numeric(df[temp_col], errors='coerce')\n",
    "\n",
    "# Lo·∫°i b·ªè missing values\n",
    "df = df.dropna()\n",
    "df = df.sort_values(date_col)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nShape sau khi x·ª≠ l√Ω: {df.shape}\")\n",
    "print(f\"Th·ªëng k√™ nhi·ªát ƒë·ªô:\")\n",
    "print(df[temp_col].describe())\n",
    "\n",
    "# ===== FEATURE ENGINEERING =====\n",
    "\n",
    "def create_time_series_features(df, target_col, date_col):\n",
    "    \"\"\"\n",
    "    T·∫°o c√°c features cho time series\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "\n",
    "    # 1. TIME-BASED FEATURES (ƒê·∫∑c tr∆∞ng th·ªùi gian)\n",
    "    df_features['year'] = df_features[date_col].dt.year\n",
    "    df_features['month'] = df_features[date_col].dt.month\n",
    "    df_features['day'] = df_features[date_col].dt.day\n",
    "    df_features['dayofweek'] = df_features[date_col].dt.dayofweek\n",
    "    df_features['dayofyear'] = df_features[date_col].dt.dayofyear\n",
    "    df_features['quarter'] = df_features[date_col].dt.quarter\n",
    "    df_features['weekofyear'] = df_features[date_col].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    # 2. CYCLIC FEATURES (ƒê·∫∑c tr∆∞ng tu·∫ßn ho√†n)\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['day'] / 31)\n",
    "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['day'] / 31)\n",
    "    df_features['dayofyear_sin'] = np.sin(2 * np.pi * df_features['dayofyear'] / 365)\n",
    "    df_features['dayofyear_cos'] = np.cos(2 * np.pi * df_features['dayofyear'] / 365)\n",
    "\n",
    "    # 3. LAG FEATURES (Gi√° tr·ªã qu√° kh·ª©)\n",
    "    for lag in [1, 2, 3, 7, 14, 30]:\n",
    "        df_features[f'temp_lag_{lag}'] = df_features[target_col].shift(lag)\n",
    "\n",
    "    # 4. ROLLING WINDOW FEATURES (ƒê·∫∑c tr∆∞ng c·ª≠a s·ªï tr∆∞·ª£t)\n",
    "    for window in [3, 7, 14, 30]:\n",
    "        df_features[f'temp_rolling_mean_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).mean()\n",
    "        df_features[f'temp_rolling_std_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).std()\n",
    "        df_features[f'temp_rolling_min_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).min()\n",
    "        df_features[f'temp_rolling_max_{window}'] = df_features[target_col].rolling(window=window, min_periods=1).max()\n",
    "\n",
    "    # 5. EXPANDING WINDOW FEATURES (ƒê·∫∑c tr∆∞ng m·ªü r·ªông)\n",
    "    df_features['temp_expanding_mean'] = df_features[target_col].expanding(min_periods=1).mean()\n",
    "    df_features['temp_expanding_std'] = df_features[target_col].expanding(min_periods=1).std()\n",
    "\n",
    "    # 6. DIFFERENCE FEATURES (ƒê·∫∑c tr∆∞ng s·ª± thay ƒë·ªïi)\n",
    "    df_features['temp_diff_1'] = df_features[target_col].diff(1)\n",
    "    df_features['temp_diff_7'] = df_features[target_col].diff(7)\n",
    "\n",
    "    # 7. EXPONENTIAL WEIGHTED FEATURES\n",
    "    df_features['temp_ewm_mean_7'] = df_features[target_col].ewm(span=7, min_periods=1).mean()\n",
    "    df_features['temp_ewm_std_7'] = df_features[target_col].ewm(span=7, min_periods=1).std()\n",
    "\n",
    "    return df_features\n",
    "\n",
    "# T·∫°o features\n",
    "print(\"\\n===== T·∫†O FEATURES =====\")\n",
    "df_features = create_time_series_features(df, temp_col, date_col)\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng features: {len(df_features.columns)}\")\n",
    "print(f\"\\nDanh s√°ch features:\")\n",
    "for i, col in enumerate(df_features.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "# X·ª≠ l√Ω missing values\n",
    "print(f\"\\n\\nMissing values:\")\n",
    "missing_counts = df_features.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(missing_counts[missing_counts > 0].sort_values(ascending=False))\n",
    "    df_features = df_features.dropna()\n",
    "    print(f\"\\nShape sau khi x·ª≠ l√Ω missing values: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ missing values!\")\n",
    "\n",
    "# ===== CHU·∫®N B·ªä D·ªÆ LI·ªÜU CHO TRAINING =====\n",
    "\n",
    "# T√°ch features v√† target\n",
    "feature_columns = [col for col in df_features.columns if col not in [date_col, temp_col]]\n",
    "X = df_features[feature_columns]\n",
    "y = df_features[temp_col]\n",
    "\n",
    "print(f\"\\n===== CHU·∫®N B·ªä D·ªÆ LI·ªÜU =====\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Chia train/test theo th·ªùi gian (80/20)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Train date range: {df_features[date_col].iloc[0]} to {df_features[date_col].iloc[split_idx-1]}\")\n",
    "print(f\"Test date range: {df_features[date_col].iloc[split_idx]} to {df_features[date_col].iloc[-1]}\")\n",
    "\n",
    "# Chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ DataFrame ƒë·ªÉ gi·ªØ t√™n c·ªôt\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)\n",
    "\n",
    "print(\"\\n===== D·ªÆ LI·ªÜU ƒê√É CHU·∫®N B·ªä XONG =====\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "print(\"\\nM·∫´u d·ªØ li·ªáu X_train_scaled (5 d√≤ng ƒë·∫ßu):\")\n",
    "print(X_train_scaled.head())\n",
    "print(\"\\nTh·ªëng k√™ y_train:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "# Ki·ªÉm tra kh√¥ng c√≥ NaN ho·∫∑c Inf\n",
    "print(\"\\n===== KI·ªÇM TRA D·ªÆ LI·ªÜU =====\")\n",
    "print(f\"NaN trong X_train_scaled: {X_train_scaled.isnull().sum().sum()}\")\n",
    "print(f\"NaN trong X_test_scaled: {X_test_scaled.isnull().sum().sum()}\")\n",
    "print(f\"Inf trong X_train_scaled: {np.isinf(X_train_scaled.values).sum()}\")\n",
    "print(f\"Inf trong X_test_scaled: {np.isinf(X_test_scaled.values).sum()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature Engineering ho√†n th√†nh!\")\n",
    "print(\"\\nC√°c bi·∫øn ƒë√£ t·∫°o:\")\n",
    "print(\"- X_train_scaled, X_test_scaled: Features ƒë√£ chu·∫©n h√≥a\")\n",
    "print(\"- y_train, y_test: Target values\")\n",
    "print(\"- scaler: StandardScaler object\")\n",
    "print(\"- df_features: DataFrame ƒë·∫ßy ƒë·ªß v·ªõi t·∫•t c·∫£ features\")\n",
    "print(f\"- temp_col = '{temp_col}'\")\n",
    "print(f\"- date_col = '{date_col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# TRAIN LINEAR REGRESSION MODEL\n",
    "# ============================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "model_lr = LinearRegression()\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "print(\"\\n===== TRAINING LINEAR REGRESSION =====\")\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "y_pred_train = model_lr.predict(X_train_scaled)\n",
    "y_pred_test = model_lr.predict(X_test_scaled)\n",
    "\n",
    "# ============================\n",
    "# ƒê√ÅNH GI√Å M√î H√åNH\n",
    "# ============================\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\nüìä {model_name} Evaluation:\")\n",
    "    print(f\"MAE  : {mae:.4f}\")\n",
    "    print(f\"RMSE : {rmse:.4f}\")\n",
    "    print(f\"R¬≤   : {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "train_scores = evaluate_model(y_train, y_pred_train, \"Linear Regression (Train)\")\n",
    "test_scores = evaluate_model(y_test, y_pred_test, \"Linear Regression (Test)\")\n",
    "\n",
    "# ============================\n",
    "# V·∫º BI·ªÇU ƒê·ªí D·ª∞ B√ÅO\n",
    "# ============================\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"y_pred_test shape: {y_pred_test.shape}\")\n",
    "\n",
    "print(\"--- Chu·∫©n b·ªã v·∫Ω bi·ªÉu ƒë·ªì... ---\")\n",
    "# %matplotlib inline\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_test.values, label=\"Actual\", color=\"tab:blue\")\n",
    "plt.plot(y_pred_test, label=\"Predicted\", color=\"tab:orange\", alpha=0.8)\n",
    "plt.title(\"üìà Linear Regression - Actual vs Predicted (Test set)\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Minimum Temperature (¬∞C)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# KI·ªÇM TRA C√ÅC H·ªÜ S·ªê (COEFFICIENTS)\n",
    "# ============================\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X_train_scaled.columns,\n",
    "    \"Coefficient\": model_lr.coef_\n",
    "}).sort_values(\"Coefficient\", key=lambda x: abs(x), ascending=False)\n",
    "\n",
    "print(\"\\n===== TOP 10 FEATURES QUAN TR·ªåNG =====\")\n",
    "print(coef_df.head(10))\n",
    "\n",
    "# ============================\n",
    "# K·∫æT LU·∫¨N T·∫†M TH·ªúI\n",
    "# ============================\n",
    "\n",
    "print(\"\\n‚úÖ Linear Regression training completed!\")\n",
    "print(\"- M√¥ h√¨nh tuy·∫øn t√≠nh ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu, nhanh hu·∫•n luy·ªán.\")\n",
    "print(\"- Tuy nhi√™n, v·ªõi d·ªØ li·ªáu Time Series c√≥ t√≠nh phi tuy·∫øn cao (seasonality r√µ r·ªát), LR c√≥ th·ªÉ d·ª± b√°o ch∆∞a t·ªët ·ªü c√°c giai ƒëo·∫°n dao ƒë·ªông m·∫°nh.\")\n",
    "print(\"- B∆∞·ªõc ti·∫øp theo: hu·∫•n luy·ªán Random Forest v√† XGBoost ƒë·ªÉ so s√°nh.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# TRAIN & EVALUATE RANDOM FOREST\n",
    "# ============================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (N·∫øu ·ªü tr√™n b·∫°n ƒë√£ c√≥ evaluate_model th√¨ ƒëo·∫°n d∆∞·ªõi s·∫Ω ghi ƒë√® c√πng logic ‚Äî kh√¥ng sao)\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\nüìä {model_name} Evaluation:\")\n",
    "    print(f\"MAE  : {mae:.4f}\")\n",
    "    print(f\"RMSE : {rmse:.4f}\")\n",
    "    print(f\"R¬≤   : {r2:.4f}\")\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Kh·ªüi t·∫°o RF (m·∫∑c ƒë·ªãnh ƒë√£ kh√° m·∫°nh; c√≥ th·ªÉ tune sau)\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n===== TRAINING RANDOM FOREST =====\")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "y_pred_train_rf = rf.predict(X_train_scaled)\n",
    "y_pred_test_rf  = rf.predict(X_test_scaled)\n",
    "\n",
    "# ƒê√°nh gi√°\n",
    "train_scores_rf = evaluate_model(y_train, y_pred_train_rf, \"Random Forest (Train)\")\n",
    "test_scores_rf  = evaluate_model(y_test,  y_pred_test_rf,  \"Random Forest (Test)\")\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì d·ª± b√°o\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_test.values, label=\"Actual\", color=\"tab:blue\")\n",
    "plt.plot(y_pred_test_rf, label=\"RF Predicted\", color=\"tab:green\", alpha=0.9)\n",
    "plt.title(\"üå≤ Random Forest - Actual vs Predicted (Test set)\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Minimum Temperature (¬∞C)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# FEATURE IMPORTANCE\n",
    "# ============================\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "fi_df = pd.DataFrame({\n",
    "    \"Feature\": X_train_scaled.columns,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\n===== TOP 15 FEATURE IMPORTANCE (RF) =====\")\n",
    "print(fi_df.head(15))\n",
    "\n",
    "# V·∫Ω Top 15 features\n",
    "topk = 15\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(fi_df[\"Feature\"].head(topk)[::-1], fi_df[\"Importance\"].head(topk)[::-1])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Random Forest - Top Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Random Forest training completed!\")\n",
    "print(\"- RF h·ªçc t·ªët quan h·ªá phi tuy·∫øn/phi tham s·ªë, th∆∞·ªùng c·∫£i thi·ªán l·ªói so v·ªõi Linear Regression.\")\n",
    "print(\"- C√≥ th·ªÉ tune nhanh: n_estimators, max_depth, min_samples_leaf ƒë·ªÉ t·ªëi ∆∞u th√™m.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# TRAIN & EVALUATE XGBOOST + SUMMARY\n",
    "# ============================\n",
    "\n",
    "# C√†i n·∫øu thi·∫øu: pip install xgboost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"‚ö†Ô∏è Ch∆∞a c√†i xgboost. Ch·∫°y: pip install xgboost\")\n",
    "\n",
    "# Kh·ªüi t·∫°o XGBoost (tham s·ªë an to√†n, d·ªÖ h·ªôi t·ª•; c√≥ th·ªÉ tune th√™m)\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "print(\"\\n===== TRAINING XGBOOST =====\")\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "y_pred_train_xgb = xgb.predict(X_train_scaled)\n",
    "y_pred_test_xgb  = xgb.predict(X_test_scaled)\n",
    "\n",
    "# ƒê√°nh gi√°\n",
    "train_scores_xgb = evaluate_model(y_train, y_pred_train_xgb, \"XGBoost (Train)\")\n",
    "test_scores_xgb  = evaluate_model(y_test,  y_pred_test_xgb,  \"XGBoost (Test)\")\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì d·ª± b√°o ri√™ng XGBoost\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_test.values, label=\"Actual\", color=\"tab:blue\")\n",
    "plt.plot(y_pred_test_xgb, label=\"XGBoost Predicted\", color=\"tab:red\", alpha=0.9)\n",
    "plt.title(\"‚ö° XGBoost - Actual vs Predicted (Test set)\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Minimum Temperature (¬∞C)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So s√°nh t·ªïng h·ª£p 3 m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SO S√ÅNH T·ªîNG H·ª¢P 3 M√î H√åNH\n",
    "# ============================\n",
    "\n",
    "# ƒë·∫£m b·∫£o ƒë√£ c√≥ c√°c bi·∫øn d·ª± ƒëo√°n t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc:\n",
    "# - y_pred_test (Linear Regression)\n",
    "# - y_pred_test_rf (Random Forest)\n",
    "# - y_pred_test_xgb (XGBoost)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def metrics_dict(name, y_true, y_pred):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"R2\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "summary_rows = [\n",
    "    metrics_dict(\"Linear Regression\", y_test, y_pred_test),\n",
    "    metrics_dict(\"Random Forest\",     y_test, y_pred_test_rf),\n",
    "    metrics_dict(\"XGBoost\",           y_test, y_pred_test_xgb),\n",
    "]\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"RMSE\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== So s√°nh t·ªïng h·ª£p (th·∫•p h∆°n ‚Üí t·ªët h∆°n) ===\")\n",
    "print(summary_df)\n",
    "\n",
    "split_date = pd.to_datetime(\"1989-01-06\")\n",
    "test_idx = df['Date'] > split_date\n",
    "\n",
    "# L·∫•y Date c·ªßa t·∫≠p test\n",
    "test_dates = pd.to_datetime(df.loc[test_idx, 'Date']).reset_index(drop=True)\n",
    "\n",
    "# T·∫°o DataFrame chu·∫©n ƒë·ªÉ so s√°nh/zoom\n",
    "y_test_df = pd.DataFrame({\n",
    "    \"Date\":   test_dates,\n",
    "    \"Actual\": y_test.values,              # gi√° tr·ªã th·∫≠t c·ªßa t·∫≠p test\n",
    "    \"Linear Regression\": y_pred_test,     # d·ª± b√°o LR\n",
    "    \"Random Forest\":     y_pred_test_rf,  # d·ª± b√°o RF\n",
    "    \"XGBoost\":           y_pred_test_xgb, # d·ª± b√°o XGB\n",
    "}).set_index(\"Date\").sort_index()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(y_test_df.index, y_test_df[\"Actual\"], label=\"Actual\", linewidth=2)\n",
    "plt.plot(y_test_df.index, y_test_df[\"Linear Regression\"], label=\"Linear Regression\")\n",
    "plt.plot(y_test_df.index, y_test_df[\"Random Forest\"],     label=\"Random Forest\")\n",
    "plt.plot(y_test_df.index, y_test_df[\"XGBoost\"],           label=\"XGBoost\")\n",
    "plt.title(\"So s√°nh d·ª± b√°o tr√™n t·∫≠p Test (1989‚Äì1990)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Minimum Temperature (¬∞C)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "subset = y_test_df.loc[\"1989-07-01\":\"1989-07-07\"]\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(subset.index, subset[\"Actual\"], label=\"Actual\", linewidth=2)\n",
    "plt.plot(subset.index, subset[\"Linear Regression\"], label=\"Linear Regression\")\n",
    "plt.plot(subset.index, subset[\"Random Forest\"],     label=\"Random Forest\")\n",
    "plt.plot(subset.index, subset[\"XGBoost\"],           label=\"XGBoost\")\n",
    "plt.title(\"Zoom 1 tu·∫ßn: 1989-07-01 ‚Üí 1989-07-07\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Minimum Temperature (¬∞C)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "subset = y_test_df.loc[\"1989-08-01\":\"1989-08-31\"]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(subset.index, subset[\"Actual\"], label=\"Actual\", linewidth=2)\n",
    "plt.plot(subset.index, subset[\"Linear Regression\"], label=\"Linear Regression\")\n",
    "plt.plot(subset.index, subset[\"Random Forest\"],     label=\"Random Forest\")\n",
    "plt.plot(subset.index, subset[\"XGBoost\"],           label=\"XGBoost\")\n",
    "plt.title(\"Zoom 1 th√°ng: 1989-08\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Minimum Temperature (¬∞C)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Di·ªÖn gi·∫£i cho blog (in ra console)\n",
    "best = summary_df.iloc[0][\"Model\"]\n",
    "print(f\"- {best} ƒëang c√≥ RMSE th·∫•p nh·∫•t trong 3 m√¥ h√¨nh.\")\n",
    "print(\"- Linear Regression nhanh, d·ªÖ gi·∫£i th√≠ch nh∆∞ng h·∫°n ch·∫ø v·ªõi quan h·ªá phi tuy·∫øn.\")\n",
    "print(\"- Random Forest h·ªçc phi tuy·∫øn t·ªët, √≠t c·∫ßn chu·∫©n h√≥a, th∆∞·ªùng ·ªïn ƒë·ªãnh.\")\n",
    "print(\"- XGBoost th∆∞·ªùng th·∫Øng nh·ªù boosting, nh∆∞ng c·∫ßn ch·ªânh tham s·ªë (n_estimators, learning_rate, max_depth...).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [WIP][Optional] Interactive Dashboard\n",
    "Ph·∫ßn n√†y m√¨nh t√¨m hi·ªÉu th√™m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My document:\n",
    "- [Data Analyst & Create Interactive Dashboard for Temperature Data](https://concrete-tray-472.notion.site/Data-Analyst-Create-Interactive-Dashboard-for-Temperature-Data-2900730a967380c8a8f3cb7f8463b01d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U gradio pandas numpy scikit-learn xgboost plotly shap matplotlib statsmodels\n",
    "%pip -q install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. IMPORT & C·∫§U H√åNH ---\n",
    "%pip install -U gradio pandas numpy scikit-learn xgboost plotly shap matplotlib statsmodels\n",
    "%pip install nest_asyncio\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "\n",
    "# √Åp d·ª•ng b·∫£n v√° cho asyncio, r·∫•t quan tr·ªçng khi ch·∫°y Gradio trong Jupyter/Colab\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# --- 2. C√ÅC H√ÄM BACKEND ---\n",
    "\n",
    "def create_time_series_features(df):\n",
    "    df_feat = df.copy()\n",
    "    df_feat['month'] = df_feat.index.month\n",
    "    df_feat['dayofyear'] = df_feat.index.dayofyear\n",
    "    df_feat['weekofyear'] = df_feat.index.isocalendar().week.astype(int)\n",
    "    df_feat['dayofweek'] = df_feat.index.dayofweek\n",
    "    df_feat['quarter'] = df_feat.index.quarter\n",
    "    df_feat['year'] = df_feat.index.year\n",
    "    df_feat['lag_1'] = df_feat['temp'].shift(1)\n",
    "    df_feat['lag_7'] = df_feat['temp'].shift(7)\n",
    "    df_feat['rolling_mean_7'] = df_feat['temp'].shift(1).rolling(window=7).mean()\n",
    "    df_feat = df_feat.dropna()\n",
    "    return df_feat\n",
    "\n",
    "def train_model_and_predict(df_raw):\n",
    "    df_featured = create_time_series_features(df_raw)\n",
    "    split_date = '1990-01-01'\n",
    "    train = df_featured.loc[df_featured.index < split_date]\n",
    "    test = df_featured.loc[df_featured.index >= split_date]\n",
    "    if train.empty or test.empty:\n",
    "        raise ValueError(\"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ chia t·∫≠p train/test (c·∫ßn d·ªØ li·ªáu c·∫£ tr∆∞·ªõc v√† sau 1990-01-01).\")\n",
    "    X_train, y_train = train.drop('temp', axis=1), train['temp']\n",
    "    model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, objective='reg:squarederror', eval_metric='rmse')\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    predictions = model.predict(test.drop('temp', axis=1))\n",
    "    results_df = pd.DataFrame({'Actual': test['temp'], 'Predicted': predictions})\n",
    "    results_df['Error'] = results_df['Actual'] - results_df['Predicted']\n",
    "    return model, X_train, df_featured, results_df\n",
    "\n",
    "# --- 3. C√ÅC H√ÄM T∆Ø∆†NG T√ÅC V·ªöI GIAO DI·ªÜN GRADIO ---\n",
    "\n",
    "def process_uploaded_file(uploaded_file):\n",
    "    if uploaded_file is None:\n",
    "        return (gr.update(visible=False), None, gr.update(value=\"Vui l√≤ng t·∫£i l√™n file CSV.\", visible=True), None, None)\n",
    "    try:\n",
    "        df_raw = pd.read_csv(uploaded_file.name)\n",
    "        df_raw.columns = ['date', 'temp']\n",
    "        df_raw = df_raw[~df_raw['temp'].str.contains('[^0-9.-]', na=False)]\n",
    "        df_raw['date'] = pd.to_datetime(df_raw['date'])\n",
    "        df_raw['temp'] = df_raw['temp'].astype(float)\n",
    "        df_raw = df_raw.sort_values('date').set_index('date')\n",
    "        model, X_train, df_featured, results_df = train_model_and_predict(df_raw)\n",
    "        processed_data = {\"model\": model, \"X_train\": X_train, \"df_featured\": df_featured, \"results_df\": results_df}\n",
    "        min_date, max_date = results_df.index.min().strftime('%Y-%m-%d'), results_df.index.max().strftime('%Y-%m-%d')\n",
    "        return (gr.update(visible=True), processed_data, gr.update(value=\"‚úÖ **File ƒë√£ x·ª≠ l√Ω!**\", visible=True), gr.update(value=min_date), gr.update(value=max_date))\n",
    "    except Exception as e:\n",
    "        return (gr.update(visible=False), None, gr.update(value=f\"‚ùå **L·ªói:** {e}\", visible=True), None, None)\n",
    "\n",
    "def update_all_tabs(start_date, end_date, processed_data):\n",
    "    if processed_data is None: return tuple([\"N/A\"] * 3 + [None] * 9)\n",
    "    try:\n",
    "        results_df, df_featured = processed_data[\"results_df\"], processed_data[\"df_featured\"]\n",
    "        start_ts, end_ts = pd.to_datetime(start_date), pd.to_datetime(end_date)\n",
    "        filtered_results = results_df[(results_df.index >= start_ts) & (results_df.index <= end_ts)]\n",
    "        if filtered_results.empty: raise ValueError(\"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng th·ªùi gian n√†y.\")\n",
    "\n",
    "        mae = mean_absolute_error(filtered_results['Actual'], filtered_results['Predicted'])\n",
    "        rmse = np.sqrt(mean_squared_error(filtered_results['Actual'], filtered_results['Predicted']))\n",
    "        r2 = r2_score(filtered_results['Actual'], filtered_results['Predicted'])\n",
    "        fig_actual_vs_pred = px.line(filtered_results, y=['Actual', 'Predicted'], title=\"Th·ª±c t·∫ø vs. D·ª± b√°o\")\n",
    "        rolling_mae = filtered_results['Error'].abs().rolling(window=7).mean()\n",
    "        fig_mae_time = px.line(rolling_mae, title=\"MAE tr∆∞·ª£t 7 ng√†y\")\n",
    "        mean_error, std_error, max_error = filtered_results['Error'].mean(), filtered_results['Error'].std(), filtered_results['Error'].abs().max()\n",
    "        fig_error_time = px.line(filtered_results, y='Error', title=\"Sai s·ªë theo th·ªùi gian\")\n",
    "        fig_hist_error = px.histogram(filtered_results, x='Error', nbins=30, title=\"Ph√¢n ph·ªëi Sai s·ªë\")\n",
    "        fig_scatter = px.scatter(filtered_results, x='Actual', y='Predicted', title=\"T∆∞∆°ng quan Th·ª±c t·∫ø vs. D·ª± b√°o\", trendline='ols')\n",
    "        fig_drift = go.Figure()\n",
    "        fig_drift.add_trace(go.Histogram(x=df_featured.loc[df_featured.index < '1990-01-01', 'temp'], name='Training Data', histnorm='probability density', opacity=0.7))\n",
    "        fig_drift.add_trace(go.Histogram(x=filtered_results['Actual'], name='Current Data', histnorm='probability density', opacity=0.7))\n",
    "        fig_drift.update_layout(barmode='overlay', title_text='So s√°nh Ph√¢n ph·ªëi D·ªØ li·ªáu')\n",
    "\n",
    "        return (f\"{mae:.2f}¬∞C\", f\"{rmse:.2f}¬∞C\", f\"{r2:.2f}\", fig_actual_vs_pred, fig_mae_time, f\"{mean_error:.2f}¬∞C\", f\"{std_error:.2f}¬∞C\", f\"{max_error:.2f}¬∞C\", fig_error_time, fig_hist_error, fig_scatter, fig_drift)\n",
    "    except Exception as e:\n",
    "        error_fig = go.Figure(layout={\"title\": f\"L·ªói: {e}\"})\n",
    "        return tuple([\"L·ªói\"] * 3 + [error_fig] * 9)\n",
    "\n",
    "# ======================================================================\n",
    "# >>> PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG NH·∫§T <<<\n",
    "def handle_tab_selection(processed_data, evt: gr.SelectData):\n",
    "    \"\"\"\n",
    "    H√†m x·ª≠ l√Ω s·ª± ki·ªán ch·ªçn tab.\n",
    "    N√≥ CH·ªà t√≠nh to√°n SHAP khi tab 'Di·ªÖn gi·∫£i M√¥ h√¨nh' ƒë∆∞·ª£c ch·ªçn.\n",
    "    \"\"\"\n",
    "    if evt.index == 3:  # Index c·ªßa Tab 'Di·ªÖn gi·∫£i M√¥ h√¨nh'\n",
    "        if processed_data is None:\n",
    "            fig, ax = plt.subplots(); ax.text(0.5, 0.5, \"Vui l√≤ng t·∫£i v√† x·ª≠ l√Ω file tr∆∞·ªõc.\", ha='center'); return fig\n",
    "        try:\n",
    "            model = processed_data[\"model\"]\n",
    "            # L·∫•y DataFrame t·ª´ state v√† KH√îI PH·ª§C KI·ªÇU D·ªÆ LI·ªÜU\n",
    "            X_train = processed_data[\"X_train\"].astype(float)\n",
    "\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            shap.summary_plot(shap_values, X_train, plot_type=\"bar\", show=False)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "        except Exception as e:\n",
    "            fig, ax = plt.subplots(); ax.text(0.5, 0.5, f\"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì SHAP:\\n{e}\", ha='center', wrap=True); return fig\n",
    "    # N·∫øu kh√¥ng ph·∫£i tab SHAP, kh√¥ng l√†m g√¨ c·∫£\n",
    "    return gr.update()\n",
    "# >>> K·∫æT TH√öC PH·∫¶N S·ª¨A L·ªñI <<<\n",
    "# ======================================================================\n",
    "\n",
    "# --- 4. X√ÇY D·ª∞NG GIAO DI·ªÜN ---\n",
    "with gr.Blocks(theme='soft', title=\"Dashboard D·ª± b√°o Nhi·ªát ƒë·ªô\") as demo:\n",
    "    processed_data_state = gr.State()\n",
    "    gr.Markdown(\"# üå§Ô∏è Dashboard Gi√°m s√°t D·ª± b√°o Nhi·ªát ƒë·ªô T·ªëi thi·ªÉu\")\n",
    "    file_uploader = gr.File(label=\"B∆∞·ªõc 1: T·∫£i l√™n file 'timeseries_daily-minimum-temperatures.csv'\", file_types=[\".csv\"])\n",
    "    status_markdown = gr.Markdown(\"Vui l√≤ng t·∫£i l√™n file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.\")\n",
    "    with gr.Column(visible=False) as main_dashboard:\n",
    "        gr.Markdown(\"---\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"#### B∆∞·ªõc 2: B·ªô l·ªçc\")\n",
    "                start_date_picker = gr.Textbox(label=\"Ng√†y b·∫Øt ƒë·∫ßu (YYYY-MM-DD)\")\n",
    "                end_date_picker = gr.Textbox(label=\"Ng√†y k·∫øt th√∫c (YYYY-MM-DD)\")\n",
    "                update_button = gr.Button(\"C·∫≠p nh·∫≠t Dashboard\", variant=\"primary\")\n",
    "            with gr.Column(scale=3):\n",
    "                with gr.Tabs() as tabs:\n",
    "                    with gr.TabItem(\"üìä 1. T·ªïng quan Hi·ªáu su·∫•t\", id=0):\n",
    "                        with gr.Row(): mae_kpi, rmse_kpi, r2_kpi = [gr.Textbox(label=l, interactive=False) for l in [\"MAE\", \"RMSE\", \"R¬≤ Score\"]]\n",
    "                        plot_actual_pred = gr.Plot()\n",
    "                        plot_mae_time = gr.Plot()\n",
    "                    with gr.TabItem(\"üîç 2. Ph√¢n t√≠ch L·ªói\", id=1):\n",
    "                        with gr.Row(): mean_error_kpi, std_error_kpi, max_error_kpi = [gr.Textbox(label=l, interactive=False) for l in [\"Mean Error (Bias)\", \"Std Dev of Error\", \"Max Absolute Error\"]]\n",
    "                        plot_error_time = gr.Plot()\n",
    "                        with gr.Row(): plot_hist_error = gr.Plot(); plot_scatter = gr.Plot()\n",
    "                    with gr.TabItem(\"üìâ 3. Data & Prediction Drift\", id=2):\n",
    "                        plot_drift = gr.Plot()\n",
    "                    with gr.TabItem(\"üß† 6. Di·ªÖn gi·∫£i M√¥ h√¨nh\", id=3):\n",
    "                        plot_shap = gr.Plot()\n",
    "    # --- 5. K·∫æT N·ªêI T∆Ø∆†NG T√ÅC ---\n",
    "    all_outputs = [mae_kpi, rmse_kpi, r2_kpi, plot_actual_pred, plot_mae_time, mean_error_kpi, std_error_kpi, max_error_kpi, plot_error_time, plot_hist_error, plot_scatter, plot_drift]\n",
    "    file_uploader.upload(fn=process_uploaded_file, inputs=file_uploader, outputs=[main_dashboard, processed_data_state, status_markdown, start_date_picker, end_date_picker])\n",
    "    update_button.click(fn=update_all_tabs, inputs=[start_date_picker, end_date_picker, processed_data_state], outputs=all_outputs)\n",
    "    tabs.select(fn=handle_tab_selection, inputs=processed_data_state, outputs=plot_shap)\n",
    "# --- 6. KH·ªûI CH·∫†Y DASHBOARD ---\n",
    "demo.launch(share=True, inline=False, debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuClass": "premium",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
