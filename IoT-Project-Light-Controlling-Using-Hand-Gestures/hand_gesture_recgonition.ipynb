{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mylethidiem/artificial-intelligence-projects/blob/main/IoT%20Project%20Light%20Controlling%20Using%20Hand%20Gestures/hand_gesture_recgonition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZJH1v68YX4M"
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9QZbsqlXKvu",
    "outputId": "ca02decc-b12a-40d0-d909-28eb1db33de5"
   },
   "outputs": [],
   "source": [
    "# If not run step Prepare data\n",
    "!gdown  1gzWOtABiVmJ38usCSDe5F9gR2tECt3zu -O data/\n",
    "!gdown  15lwipssmC_K82ukRfb0uVCiDH1TZ3QCf -O data/\n",
    "!gdown  1nIo1_wBmkovz-u_BCsV5c1Kbz6ZqoKwq -O data/\n",
    "# Download file hand_gesture.yam\n",
    "!gdown  1ZteHYSgbuZu_GcUJHW8ZzoZv1DE8-oLw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-OnqatIAOf2"
   },
   "source": [
    "[MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker) is an open framework from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yjx53XG__3qR",
    "outputId": "fbbff6ac-544f-4bc5-8e94-67a9b674483e"
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe==0.10.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgouxhmTAtFN"
   },
   "source": [
    "[TorchMetrics](https://lightning.ai/docs/torchmetrics/stable/) is a collection of 100+ PyTorch metrics implementations and an easy-to-use API to create custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtNoz5fTAtoy",
    "outputId": "25f30904-0353-41d1-cc3c-f6cad3c437bc"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MARhHzLXKW4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import cv2 # OpenCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkfpjekFW0gC"
   },
   "outputs": [],
   "source": [
    "def label_dict_from_config_file(file_path):\n",
    "  label_tag = []\n",
    "  with open(file_path, 'r') as f:\n",
    "    label_tag = yaml.full_load(f)['gestures']\n",
    "  return label_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqhc2257_m3K"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    input = 63\n",
    "    hidden = 128\n",
    "    rate = [0.4, 0.6]\n",
    "    list_label = label_dict_from_config_file(\"hand_gesture.yaml\")\n",
    "    output = len(list_label)\n",
    "\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(input, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(hidden),\n",
    "\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=rate[0]),\n",
    "\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=rate[0]),\n",
    "\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=rate[1]),\n",
    "\n",
    "        nn.Linear(hidden, output)\n",
    "    )\n",
    "    # q1 A\n",
    "    # for m in self.modules():\n",
    "    #   if isinstance(m, nn.Linear):\n",
    "    #     nn.init.normal_(m.weight, mean=0.0, std=0.05)\n",
    "    #     nn.init.constant_(m.bias, val=0.0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x  = self.flatten(x)\n",
    "    #q2 A\n",
    "    output = self.linear_relu_stack(x)\n",
    "    return output\n",
    "\n",
    "  def predict(self, x, threshold=0.8):\n",
    "    logits = self(x)\n",
    "    softmax_prob = nn.Softmax(dim=1)(logits)\n",
    "    chosen_ind = torch.argmax(softmax_prob, dim=1)\n",
    "    return torch.where(softmax_prob[0, chosen_ind] > threshold, chosen_ind, -1)\n",
    "\n",
    "  def predict_with_known_class(self, x):\n",
    "    logits = self(x)\n",
    "    softmax_prob = nn.Softmax(dim=1)(logits)\n",
    "    return torch.argmax(softmax_prob, dim=1)\n",
    "\n",
    "  def score(self, logits):\n",
    "    return -torch.amax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCrCRegTE2M0"
   },
   "outputs": [],
   "source": [
    "class HanddLandmarkDetector():\n",
    "  def __init__(self) -> None:\n",
    "    # drawing onto image\n",
    "    self.mp_drawing_utils = mp.solutions.mp_drawing_utils\n",
    "    self.mp_drawing_styles = mp.solutions.mp_drawing_styles\n",
    "\n",
    "    # detector hand gestures\n",
    "    self.mp_hands = mp.solutions.hands\n",
    "    self.detector = self.mp_hands.Hands(static_image_mode=False,\n",
    "                                        max_num_hands=1,\n",
    "                                        min_detection_confidence=0.5)\n",
    "\n",
    "  def detect_hand(self, frame):\n",
    "    hands = []\n",
    "\n",
    "    # flip the frame horizontally (mirror effect)\n",
    "    frame = cv2.flip(frame, flipCode=1)\n",
    "\n",
    "    annotated_image = frame.copy()\n",
    "\n",
    "    results = self.detector.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if results.multi_hand_landmarks is not None:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        hand = []\n",
    "\n",
    "        # draw hand landmarks into image\n",
    "        self.mp_drawing_utils(\n",
    "            image=annotated_image,\n",
    "            landmark_list=hand_landmarks,\n",
    "            connection=self.mp_hands.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=self.mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            connection_drawing_spec=self.mp_drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "          x, y, z = landmark.x, landmark.y, landmark.z\n",
    "          hand.extend(x, y, z)\n",
    "        hands.append(hand)\n",
    "    return hands, annotated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHwJCV7PcWAh"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    # Custom dataset class for loading images and their labels\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        # Initialize the dataset\n",
    "        self.data = pd.read_csv(data_file)  # Read the data file (CSV) containing image data and labels into a pandas DataFrame\n",
    "        self.labels = torch.from_numpy(self.data.iloc[:, 0].to_numpy())  # Convert the first column (labels) to a NumPy array and then to a PyTorch tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.data)  # Return the length of the data DataFrame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a sample from the dataset\n",
    "        one_hot_label = self.labels[idx]  # Get the label for the given index\n",
    "        torch_data = torch.from_numpy(self.data.iloc[idx, 1:].to_numpy(dtype=np.float32))  # Convert the image data (all columns except the first) to a NumPy array and then to a PyTorch tensor\n",
    "        return torch_data, one_hot_label  # Return the image data and label as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cfE9RMNYqnd"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # Class for early stopping during training\n",
    "\n",
    "    def __init__(self, patience=1, min_delta=0.0):\n",
    "        # Initialize the early stopper\n",
    "        self.patience = patience  # Number of epochs to wait before stopping\n",
    "        self.min_delta = min_delta  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "        self.counter = 0  # Counter for the number of epochs without improvement\n",
    "        self.watched_metrics = np.inf  # Initialize the watched metric to infinity\n",
    "\n",
    "    def early_stop(self, current_value):\n",
    "        # Check if training should be stopped\n",
    "        if current_value < self.watched_metrics:\n",
    "            # If the current value is better (lower) than the watched metric\n",
    "            self.watched_metrics = current_value  # Update the watched metric\n",
    "            self.counter = 0  # Reset the counter\n",
    "        elif current_value > (self.watched_metrics + self.min_delta):\n",
    "            # If the current value is worse (higher) than the watched metric plus the minimum delta\n",
    "            self.counter += 1  # Increment the counter\n",
    "            if self.counter >= self.patience:\n",
    "                # If the counter exceeds the patience threshold\n",
    "                return True  # Stop training\n",
    "        return False  # Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hx7VbqqKZI3w"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, loss_function, optimizer, early_stopper):\n",
    "  # add auroc score\n",
    "  epochs = 300\n",
    "  best_vloss = 1_000_000\n",
    "  timestamp = datetime.now().strftime('%d-%m %H:%M')\n",
    "  best_model_path = ''\n",
    "  train_loss_list = []\n",
    "  train_acc_list = []\n",
    "  val_loss_list = []\n",
    "  val_acc_list = []\n",
    "\n",
    "  for epoch in range(300):\n",
    "    #training step\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    acc_train = Accuracy(num_classes=len(LIST_LABEL), task='MULTICLASS')\n",
    "    for batch_number, data in enumerate(train_loader):\n",
    "      inputs, labels = data\n",
    "\n",
    "      #q9 A\n",
    "      optimizer.zero_grad()\n",
    "      preds = model(inputs)\n",
    "\n",
    "      #q10 A\n",
    "      loss = loss_function(preds, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      acc_train.update(model.predict_with_known_class(inputs), labels) #\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    #validating step\n",
    "    model.train(False)\n",
    "    running_vloss = 0.0\n",
    "    acc_val = Accuracy(num_classes=len(LIST_LABEL), task='MULTICLASS')\n",
    "    for i, vdata in enumerate(val_loader):\n",
    "      vinputs, vlabels = vdata\n",
    "      preds = model(vinputs)\n",
    "      vloss = loss_function(preds, vlabels)\n",
    "      running_vloss += vloss.item()\n",
    "      acc_val.update(model.predict_with_known_class(vinputs), vlabels)\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    print(f\"Epoch {epoch}/{epochs}:\")\n",
    "    print(f\"Accuracy train: {acc_train.compute().item()}, val:{acc_val.compute().item()}\")\n",
    "\n",
    "    avg_vloss = running_vloss / len(val_loader)\n",
    "    train_loss_list.append(avg_loss)\n",
    "    train_acc_list.append(acc_train.compute().item())\n",
    "    val_loss_list.append(avg_vloss)\n",
    "    val_acc_list.append(acc_val.compute().item())\n",
    "\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('Training vs. Validation Loss',\n",
    "          {'Training' : avg_loss, 'Validation': avg_vloss},\n",
    "          epoch + 1)\n",
    "    print('Training vs. Validation accuracy',\n",
    "          {'Training' : acc_train.compute().item(),\n",
    "           'Validation' : acc_val.compute().item()},\n",
    "          epoch + 1)\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "      best_vloss = avg_vloss\n",
    "      best_model_path = f'./{save_path}/model_{timestamp}_{model.__class__.__name__}_best'\n",
    "      torch.save(model.state_dict(), best_model_path) # Save the model's state\n",
    "\n",
    "    #q5 A\n",
    "    if early_stopper.early_stop(avg_vloss):\n",
    "      print(f\"Stopping at {epoch}, minium: {early_stopper.watched_metrics}\")\n",
    "      break\n",
    "\n",
    "  model_path = f'./{save_path}/model_{timestamp}_{model.__class__.__name__}_last'\n",
    "  torch.save(model.state_dict(), model_path)\n",
    "  train_val_loss_acc = [train_loss_list, train_acc_list, val_loss_list, val_acc_list]\n",
    "\n",
    "\n",
    "  return model, best_model_path, train_val_loss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K62dxGELmC6W"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH=\"./data/\"\n",
    "LIST_LABEL = label_dict_from_config_file(\"hand_gesture.yaml\")\n",
    "train_path = os.path.join(DATA_FOLDER_PATH, \"landmark_train.csv\")\n",
    "val_path = os.path.join(DATA_FOLDER_PATH, \"landmark_val.csv\")\n",
    "test_path = os.path.join(DATA_FOLDER_PATH, \"landmark_test.csv\")\n",
    "\n",
    "trainset = CustomImageDataset(train_path)\n",
    "testset = CustomImageDataset(test_path)\n",
    "valset = CustomImageDataset(os.path.join(val_path))\n",
    "\n",
    "save_path = './models'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "#q3 A\n",
    "batch_size_train = 40\n",
    "batch_size_val = 40\n",
    "#q6 A\n",
    "batch_size_test = 20\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size_test, shuffle=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size_train, shuffle=True)\n",
    "val_loader =  torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size_val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX3lixLfG2v-"
   },
   "outputs": [],
   "source": [
    "#q8 A\n",
    "model = NeuralNetwork()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "min_delta = 1e-2\n",
    "early_stopper = EarlyStopper(patience=30, min_delta=min_delta)\n",
    "\n",
    "#q4 A\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtAkOfNTG5A9",
    "outputId": "9a8cc10c-a71d-4ad5-923c-28f89ae1bc27"
   },
   "outputs": [],
   "source": [
    "model, best_model_path, train_val_loss_acc = train(train_loader, val_loader, model, loss_function, optimizer, early_stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLXMMXmaiOn9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualization(train_loss_list, train_acc_list, val_loss_list, val_acc_list):\n",
    "  fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "  ax[0,0].plot(train_loss_list, color='green')\n",
    "  ax[0,0].set(title='Train Loss', xlabel='Epoch', ylabel='Loss')\n",
    "\n",
    "  ax[0,1].plot(train_acc_list, color='green')\n",
    "  ax[0,1].set(title='Train Accuracy', xlabel='Epoch', ylabel='Accuracy')\n",
    "\n",
    "  ax[1,0].plot(val_loss_list, color='orange')\n",
    "  ax[1,0].set(title='Validation Loss', xlabel='Epoch', ylabel='Loss')\n",
    "\n",
    "  ax[1,1].plot(val_acc_list, color='orange')\n",
    "  ax[1,1].set(title='Train Accuracy', xlabel='Epoch', ylabel='Accuracy')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "5-PFKwABkoZf",
    "outputId": "ce26c71f-2f17-4a2f-f416-3e27da92deae"
   },
   "outputs": [],
   "source": [
    "train_loss_list = train_val_loss_acc[0]\n",
    "train_acc_list = train_val_loss_acc[1]\n",
    "val_loss_list = train_val_loss_acc[2]\n",
    "val_acc_list = train_val_loss_acc[3]\n",
    "\n",
    "visualization(train_loss_list, train_acc_list, val_loss_list, val_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4dLQipbzcWE",
    "outputId": "cd617863-ea65-4ac3-f870-8ff9e27d70ce"
   },
   "outputs": [],
   "source": [
    "network = NeuralNetwork()\n",
    "network.load_state_dict(torch.load(best_model_path, weights_only=False))\n",
    "\n",
    "network.eval()\n",
    "acc_test = Accuracy(num_classes=len(LIST_LABEL), task='MULTICLASS')\n",
    "# with torch.no_grad:\n",
    "for i, test_data in enumerate(test_loader):\n",
    "  test_input, test_label = test_data\n",
    "  #q7 A\n",
    "  preds = network(test_input)\n",
    "  acc_test.update(preds, test_label)\n",
    "\n",
    "print(network.__class__.__name__)\n",
    "print(f\"Accuracy of model:{acc_test.compute().item()}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
