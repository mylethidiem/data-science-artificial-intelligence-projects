{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdE3415GA72R"
   },
   "source": [
    "## **0. Tải bộ dữ liệu**\n",
    "**Lưu ý:** Nếu bạn không thể sử dụng lệnh gdown để tải bộ dữ liệu vì bị giới hạn số lượt tải, hãy tải bộ dữ liệu thử công và upload lên google drive của mình. Sau đó, sử dụng lệnh dưới đây để copy file dữ liệu vào colab:\n",
    "```python\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "!cp /path/to/dataset/on/your/drive .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHfGc0o2fZMu"
   },
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R/view?usp=sharing\n",
    "!gdown --id 1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVvC6obsBYG4"
   },
   "source": [
    "## **1. Import các thư viện cần thiết**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU6nV0YHhWWm"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX0MeDrnBcY1"
   },
   "source": [
    "## **2. Đọc bộ dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoeUNKOclJK9"
   },
   "outputs": [],
   "source": [
    "# DATASET_PATH = '/content/2cls_spam_text_cls.csv'\n",
    "DATASET_PATH = '2cls_spam_text_cls.csv'\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQblTvxTnIPj"
   },
   "outputs": [],
   "source": [
    "messages = df['Message'].values.tolist()\n",
    "labels = df['Category'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WadqE8dtBf9Q"
   },
   "source": [
    "## **3. Chuẩn bị bộ dữ liệu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2FJWXIABiwa"
   },
   "source": [
    "### **3.1. Xử lý dữ liệu nhãn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzBrlBKslOz5"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "print(f'Classes: {le.classes_}')\n",
    "print(f'Encoded labels: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7aTeSvzBogY"
   },
   "source": [
    "### **3.2. Xử lý dữ liệu đặc trưng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_m5jsZBlh5P"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# It's good practice to ensure the necessary NLTK data is downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def punctuation_removal(text):\n",
    "    \"\"\"Removes punctuation from text.\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Removes English stopwords from a list of tokens.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # The list comprehension is the most efficient way to do this.\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def stemming(tokens):\n",
    "    \"\"\"Applies Porter Stemmer to a list of tokens.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Applies a full preprocessing pipeline to a list of text documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation\n",
    "    text = punctuation_removal(text)\n",
    "\n",
    "    # 3. Tokenize the cleaned text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Remove stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "\n",
    "    # 5. Apply stemming\n",
    "    tokens = stemming(tokens)\n",
    "\n",
    "    tokens\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# --- Example Usage ---\n",
    "# sample_texts = [\n",
    "#     \"This is the first document; it is amazing!\",\n",
    "#     \"Here is the second one, which is also interesting.\"\n",
    "# ]\n",
    "\n",
    "# processed_data = preprocess_text(sample_texts)\n",
    "# print(processed_data)\n",
    "# Output: [['first', 'document', 'amaz'], ['second', 'one', 'also', 'interest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5KV1Kpwmh7y"
   },
   "outputs": [],
   "source": [
    "processed_messages = [preprocess_text(message) for message in messages]\n",
    "processed_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm_bcltsmm98"
   },
   "outputs": [],
   "source": [
    "def create_dictionary(messages):\n",
    "    \"\"\"Creates a vocabulary of unique words from a list of tokenized messages.\"\"\"\n",
    "    all_words = []\n",
    "\n",
    "    for tokens in messages:\n",
    "        # Extend the list with all tokens from the current message\n",
    "        all_words.extend(tokens)\n",
    "    print(\"all_words=\",all_words)\n",
    "    # Create a dictionary of unique words by converting to a set, then back to a list\n",
    "    dictionary = sorted(list(set(all_words)))\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def create_features(tokens, dictionary):\n",
    "    features = np.zeros(len(dictionary))\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in dictionary:\n",
    "            features[dictionary.index(token)] += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = create_dictionary(processed_messages)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEy3OrleoARz"
   },
   "outputs": [],
   "source": [
    "X = np.array([create_features(tokens, dictionary) for tokens in processed_messages])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvzSFGPyB83P"
   },
   "source": [
    "### **3.3. Chia dữ liệu train/val/test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKm5qxIWhJmT"
   },
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.125\n",
    "SEED = 0\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=VAL_SIZE,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train,\n",
    "                                                    test_size=TEST_SIZE,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FTBzozgBuIx"
   },
   "source": [
    "## **4. Huấn luyện mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anE-mJw5qndx"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = GaussianNB()\n",
    "print('Start training...')\n",
    "model = model.fit(X_train, y_train)\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCoOf4BrBwYI"
   },
   "source": [
    "## **5. Đánh giá mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9qcQ_4Pqpes"
   },
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Val accuracy: {val_accuracy}')\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1GS4YWRByPB"
   },
   "source": [
    "## **6. Thực hiện dự đoán**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BVohFfpqmJ8"
   },
   "outputs": [],
   "source": [
    "def predict(text, model, dictionary):\n",
    "    processed_text = preprocess_text(text)\n",
    "    features = create_features(text, dictionary)\n",
    "    features = np.array(features).reshape(1, -1)\n",
    "    prediction = model.predict(features)\n",
    "    prediction_cls = le.inverse_transform(prediction)[0]\n",
    "\n",
    "    return prediction_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaL0bg3io8a2"
   },
   "outputs": [],
   "source": [
    "test_input = 'I am actually thinking a way of doing something useful'\n",
    "prediction_cls = predict(test_input, model, dictionary)\n",
    "print(f'Prediction: {prediction_cls}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
